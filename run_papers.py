#!/usr/bin/env python3
"""
é€šç”¨è®ºæ–‡å¤„ç† runner
è¢« run_daily.py / run_monthly.py / main.py(weekly) å…±ç”¨
"""
import os, sys, json, time
from datetime import datetime
from pathlib import Path

BASE_DIR   = os.path.dirname(os.path.abspath(__file__))
DATA_DIR   = os.path.join(BASE_DIR, "data")   # data/daily/ data/weekly/ data/monthly/
LOGS_DIR   = os.path.join(BASE_DIR, "logs")
sys.path.insert(0, BASE_DIR)


def setup_dirs(mode, key):
    """åˆ›å»ºç›®å½• data/<mode>/<key>/papers/"""
    base = os.path.join(DATA_DIR, mode, key)
    papers_dir = os.path.join(base, "papers")
    os.makedirs(papers_dir, exist_ok=True)
    os.makedirs(LOGS_DIR, exist_ok=True)
    return base, papers_dir


def get_log_file(mode, key):
    return os.path.join(LOGS_DIR, f"{mode}-{key}.log")


def log(msg, mode=None, key=None, also_print=True):
    ts   = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    line = f"[{ts}] {msg}"
    if also_print:
        print(line, flush=True)
    if mode and key:
        with open(get_log_file(mode, key), "a", encoding="utf-8") as f:
            f.write(line + "\n")


def save_index(base_dir, mode, key, papers_data, extra=None):
    idx = {
        "mode": mode,
        "key": key,
        "generated_at": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        "total": len(papers_data),
        "papers": papers_data,
    }
    if extra:
        idx.update(extra)
    idx_file = os.path.join(base_dir, "index.json")
    with open(idx_file, "w", encoding="utf-8") as f:
        json.dump(idx, f, ensure_ascii=False, indent=2)
    return idx_file


def run(mode, key, limit, do_full_translate=False):
    """
    ä¸»æµç¨‹
    mode:  'daily' | 'weekly' | 'monthly'
    key:   æ—¥æœŸå­—ç¬¦ä¸²
    limit: è®ºæ–‡æ•°ä¸Šé™
    """
    print("=" * 60, flush=True)
    print(f"ğŸ“š Paper Trans â€” {mode.upper()} {key}", flush=True)
    print("=" * 60, flush=True)

    log(f"å¼€å§‹: {mode} {key}", mode, key)

    from fetch_hf import fetch_hf_papers
    from translate_arxiv import load_api_config, translate_and_save

    base_dir, papers_dir = setup_dirs(mode, key)
    log(f"ğŸ“ {base_dir}", mode, key)

    # 1. æŠ“å–
    papers = fetch_hf_papers(mode, key, limit)
    if not papers:
        log("âŒ æœªè·å–åˆ°è®ºæ–‡", mode, key)
        return False

    log(f"âœ… è·å–åˆ° {len(papers)} ç¯‡", mode, key)

    # 2. API é…ç½®
    config = load_api_config()
    log(f"ğŸ“¡ æ¨¡å‹: {config['model']}", mode, key)

    # 3. é€ä¸€ç¿»è¯‘æ‘˜è¦
    papers_data = []
    ok = fail = 0

    for i, paper in enumerate(papers, 1):
        arxiv_id = paper.get("arxiv_id", "")
        if not arxiv_id:
            continue

        html_path = os.path.join(papers_dir, f"{arxiv_id}.html")
        if os.path.exists(html_path) and os.path.getsize(html_path) > 500:
            log(f"  [{i}/{len(papers)}] â­ï¸  å·²å­˜åœ¨: {arxiv_id}", mode, key)
            # ä»å·²æœ‰ index æ¢å¤
            try:
                existing = json.load(open(os.path.join(base_dir, "index.json")))
                for ep in existing.get("papers", []):
                    if ep.get("arxiv_id") == arxiv_id:
                        papers_data.append(ep)
                        ok += 1
                        break
                else:
                    papers_data.append({"arxiv_id": arxiv_id, "rank": i,
                                        "html_file": f"papers/{arxiv_id}.html"})
                    ok += 1
            except Exception:
                papers_data.append({"arxiv_id": arxiv_id, "rank": i,
                                    "html_file": f"papers/{arxiv_id}.html"})
                ok += 1
            continue

        log(f"  [{i}/{len(papers)}] ğŸ”„ ç¿»è¯‘: {arxiv_id}", mode, key)
        try:
            # week_str ä¼  "mode/key" ä½¿ HTML å†…åµŒçš„"è¿”å›"é“¾æ¥æŒ‡å‘æ­£ç¡®è·¯å¾„
            result = translate_and_save(
                arxiv_id=arxiv_id,
                output_dir=papers_dir,
                rank=i,
                week_str=f"{mode}/{key}",
                config=config,
            )
            result["rank"] = i
            result["upvotes"] = paper.get("upvotes", 0)
            result["html_file"] = f"papers/{arxiv_id}.html"
            papers_data.append(result)
            ok += 1
            log(f"  âœ… {result.get('title_zh') or result.get('title', arxiv_id)}", mode, key)
        except Exception as e:
            log(f"  âŒ {arxiv_id}: {e}", mode, key)
            papers_data.append({"arxiv_id": arxiv_id, "rank": i, "error": str(e),
                                 "html_file": f"papers/{arxiv_id}.html"})
            fail += 1

        save_index(base_dir, mode, key, papers_data)

        if i < len(papers):
            time.sleep(2)

    idx_file = save_index(base_dir, mode, key, papers_data)

    # 4. å…¨æ–‡ç¿»è¯‘ï¼ˆæ‰€æœ‰æ¨¡å¼å‡æ”¯æŒï¼Œä¼  do_full_translate=False å¯è·³è¿‡ï¼‰
    if do_full_translate:
        log("ğŸ”¬ å¼€å§‹å…¨æ–‡ç¿»è¯‘...", mode, key)
        from translate_full import translate_full
        for entry in papers_data:
            aid = entry.get("arxiv_id", "")
            if not aid:
                continue
            pdf_path = os.path.join(papers_dir, f"{aid}_zh.pdf")
            if os.path.exists(pdf_path) and os.path.getsize(pdf_path) > 10240:
                log(f"  â­ï¸  å…¨æ–‡PDFå·²å­˜åœ¨: {aid}", mode, key)
                entry["pdf_zh"] = f"papers/{aid}_zh.pdf"
                continue
            log(f"  ğŸ”¬ å…¨æ–‡ç¿»è¯‘: {aid}", mode, key)
            try:
                r = translate_full(arxiv_id=aid, output_dir=papers_dir,
                                   no_cache=False, timeout=3600)
                if r.get("pdf_path"):
                    entry["pdf_zh"] = f"papers/{aid}_zh.pdf"
                    log(f"  âœ… PDF: {r['pdf_path']}", mode, key)
                else:
                    log(f"  âŒ {r.get('error','')}", mode, key)
            except Exception as e:
                log(f"  âŒ {aid}: {e}", mode, key)
            save_index(base_dir, mode, key, papers_data)
        idx_file = save_index(base_dir, mode, key, papers_data)

    log(f"ğŸ“Š å®Œæˆ: æˆåŠŸ={ok} å¤±è´¥={fail}  {idx_file}", mode, key)
    return fail == 0
